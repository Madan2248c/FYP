{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6fa8c5",
   "metadata": {},
   "source": [
    "# GRPO Fine-tuning for AMR Prescription Validation\n",
    "\n",
    "This notebook implements Group Relative Policy Optimization (GRPO) for fine-tuning language models on AMR (Antimicrobial Resistance) prescription validation tasks.\n",
    "\n",
    "**GRPO** is a reinforcement learning method that optimizes language models by:\n",
    "- Generating multiple outputs per prompt\n",
    "- Computing rewards for each output using LLM-as-a-Judge\n",
    "- Using group-relative advantages for stable optimization\n",
    "- Updating the model to favor high-reward outputs\n",
    "\n",
    "**Dataset**: ICMR 2025 Antimicrobial Treatment Guidelines\n",
    "**Task**: Validate prescriptions with step-by-step clinical reasoning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae1ccc1",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell first!\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "if \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    print(\"üîß Installing packages for Google Colab...\")\n",
    "    !pip install -q unsloth bitsandbytes accelerate peft trl transformers datasets\n",
    "else:\n",
    "    print(\"üîß Installing packages for local environment...\")\n",
    "    !pip install -q unsloth transformers datasets groq pydantic tqdm aiohttp nest-asyncio\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8417a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "import hashlib\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from datasets import Dataset, load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"‚úÖ All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28253c3",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf53fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "OUTPUT_DIR = \"./grpo_amr_model\"\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION API CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Supabase Edge Function URL for evaluation API\n",
    "API_BASE_URL = \"https://gdpanoqcfepugqkisqhf.supabase.co/functions/v1/evaluate-prescription\"\n",
    "\n",
    "# Priority metrics for training (faster, focused on key aspects)\n",
    "PRIORITY_METRICS = ['clinical_accuracy', 'guideline_adherence', 'reasoning_completeness']\n",
    "\n",
    "# All metrics for comprehensive evaluation\n",
    "ALL_METRICS = [\n",
    "    'clinical_accuracy',\n",
    "    'guideline_adherence',\n",
    "    'reasoning_completeness',\n",
    "    'safety_awareness',\n",
    "    'decision_appropriateness',\n",
    "    'reference_accuracy'\n",
    "]\n",
    "\n",
    "# Reward weights for each metric (must sum to 1.0)\n",
    "REWARD_WEIGHTS = {\n",
    "    'clinical_accuracy': 0.25,\n",
    "    'guideline_adherence': 0.25,\n",
    "    'reasoning_completeness': 0.20,\n",
    "    'safety_awareness': 0.15,\n",
    "    'decision_appropriateness': 0.10,\n",
    "    'reference_accuracy': 0.05\n",
    "}\n",
    "\n",
    "# API Configuration\n",
    "API_CONFIG = {\n",
    "    \"max_concurrent_requests\": 50,\n",
    "    \"timeout_seconds\": 45,\n",
    "    \"max_retries\": 3,\n",
    "    \"retry_delay\": 2.0,\n",
    "    \"use_cache\": True,\n",
    "}\n",
    "\n",
    "# Evaluation frequency (evaluate every N training steps)\n",
    "EVAL_FREQUENCY = 10\n",
    "\n",
    "# ============================================================================\n",
    "# GRPO HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "GRPO_CONFIG = {\n",
    "    \"num_generations_per_prompt\": 2,\n",
    "    \"batch_size\": 2,\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"max_length\": 1024,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"kl_coef\": 0.05,\n",
    "    \"clip_range\": 0.2,\n",
    "    \"vf_coef\": 0.1,\n",
    "    \"eval_frequency\": EVAL_FREQUENCY,\n",
    "    \"use_priority_metrics\": True,\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# UNSLOTH CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LOAD_IN_4BIT = True\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "print(f\"üìç API Base URL: {API_BASE_URL}\")\n",
    "print(f\"üéØ Using Metrics: {'PRIORITY (3 metrics)' if GRPO_CONFIG['use_priority_metrics'] else 'ALL (6 metrics)'}\")\n",
    "print(f\"üìä Evaluation Frequency: Every {EVAL_FREQUENCY} steps\")\n",
    "print(f\"‚ö° Batch Size: {GRPO_CONFIG['batch_size']}\")\n",
    "print(f\"üî¢ Generations per prompt: {GRPO_CONFIG['num_generations_per_prompt']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "   os.environ[\"SUPABASE_ANON_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImdkcGFub3FjZmVwdWdxa2lzcWhmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjQ3NDQyMDcsImV4cCI6MjA4MDMyMDIwN30.S3Bsm8MDqJWOFx6vVVBislrlX09sGth1EbR1lRfPdxw\"\n",
    "   os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3facdbf9",
   "metadata": {},
   "source": [
    "## 3. Load Dataset\n",
    "\n",
    "We'll load the prepared GRPO dataset. If you haven't prepared it yet, run:\n",
    "```bash\n",
    "python prepare_grpo_dataset.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25958509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Path to prepared dataset\n",
    "data_dir = Path(\"./data\")\n",
    "train_dataset_path = data_dir / \"train_hf\"\n",
    "\n",
    "print(f\"üìÇ Loading dataset from: {train_dataset_path}\")\n",
    "\n",
    "if not train_dataset_path.exists():\n",
    "    print(\"‚ùå Dataset not found!\")\n",
    "    print(\"Please run: python prepare_grpo_dataset.py\")\n",
    "    print(\"This will convert your merged AMR dataset to GRPO format.\")\n",
    "else:\n",
    "    train_dataset = load_from_disk(str(train_dataset_path))\n",
    "    print(f\"‚úÖ Loaded {len(train_dataset)} training examples\")\n",
    "    \n",
    "    # Show dataset structure\n",
    "    print(f\"\\nüìã Dataset columns: {train_dataset.column_names}\")\n",
    "    \n",
    "    # Show example\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Example Training Instance:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    example = train_dataset[0]\n",
    "    print(f\"\\nüìù Prompt (first 300 chars):\")\n",
    "    print(example['prompt'][:300] + \"...\")\n",
    "    print(f\"\\n‚ú® Reference (first 200 chars):\")\n",
    "    print(example['reference'][:200] + \"...\")\n",
    "    print(f\"\\nüè∑Ô∏è  Task Type: {example.get('task_type', 'unknown')}\")\n",
    "    print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d92281",
   "metadata": {},
   "source": [
    "## 4. Load Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50238d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer with Unsloth (optimized for speed and memory)\n",
    "print(f\"üîÑ Loading model: {MODEL_NAME}\")\n",
    "print(f\"This may take a few minutes on first run...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect dtype\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Model and tokenizer loaded successfully!\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"   4-bit quantization: {LOAD_IN_4BIT}\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530a7aa",
   "metadata": {},
   "source": [
    "## 5. API-Integrated Reward Model\n",
    "\n",
    "This reward model integrates with your Supabase Edge Function to evaluate model outputs using LLM-as-a-Judge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc304a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters with Unsloth (super fast!)\n",
    "print(\"üîß Adding LoRA adapters with Unsloth optimization...\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ LoRA adapters added successfully!\")\n",
    "print(\"\\nüìä Trainable Parameters:\")\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIIntegratedRewardModel:\n",
    "    \"\"\"\n",
    "    Reward model that integrates with Supabase Edge Function for evaluation.\n",
    "    \n",
    "    Features:\n",
    "    - Async batch processing with concurrency control\n",
    "    - Response caching to avoid duplicate API calls\n",
    "    - Retry logic with exponential backoff\n",
    "    - Priority metrics mode for faster training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        api_base_url: str,\n",
    "        metrics: List[str],\n",
    "        weights: Dict[str, float],\n",
    "        api_config: Dict[str, Any],\n",
    "        use_cache: bool = True\n",
    "    ):\n",
    "        self.api_base_url = api_base_url\n",
    "        self.metrics = metrics\n",
    "        self.weights = weights\n",
    "        self.api_config = api_config\n",
    "        self.use_cache = use_cache\n",
    "        \n",
    "        self.cache = {} if use_cache else None\n",
    "        self.stats = {\n",
    "            'total_calls': 0,\n",
    "            'cache_hits': 0,\n",
    "            'api_errors': 0,\n",
    "            'total_api_time': 0.0\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Reward Model initialized with {len(metrics)} metrics\")\n",
    "        print(f\"   Metrics: {metrics}\")\n",
    "        print(f\"   Caching: {'Enabled' if use_cache else 'Disabled'}\")\n",
    "    \n",
    "    def _create_cache_key(self, context: Dict, model_output: str, ground_truth: str) -> str:\n",
    "        \"\"\"Create a unique cache key for an API call.\"\"\"\n",
    "        content = f\"{json.dumps(context)}|{model_output}|{ground_truth}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    async def _call_api_async(\n",
    "        self,\n",
    "        session: aiohttp.ClientSession,\n",
    "        context: Dict,\n",
    "        model_output: str,\n",
    "        ground_truth: str,\n",
    "        retry_count: int = 0\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Make async API call to evaluation endpoint.\"\"\"\n",
    "        \n",
    "        # Check cache\n",
    "        if self.use_cache:\n",
    "            cache_key = self._create_cache_key(context, model_output, ground_truth)\n",
    "            if cache_key in self.cache:\n",
    "                self.stats['cache_hits'] += 1\n",
    "                return self.cache[cache_key]\n",
    "        \n",
    "        # Prepare request\n",
    "        payload = {\n",
    "            \"patient_case\": context,\n",
    "            \"model_output\": model_output,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"metrics\": self.metrics\n",
    "        }\n",
    "        \n",
    "        # Prepare headers with Supabase authentication\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        supabase_key = os.getenv(\"SUPABASE_ANON_KEY\")\n",
    "        if supabase_key:\n",
    "            headers[\"Authorization\"] = f\"Bearer {supabase_key}\"\n",
    "            headers[\"apikey\"] = supabase_key\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            async with session.post(\n",
    "                self.api_base_url,\n",
    "                json=payload,\n",
    "                headers=headers,\n",
    "                timeout=aiohttp.ClientTimeout(total=self.api_config['timeout_seconds'])\n",
    "            ) as response:\n",
    "                elapsed = time.time() - start_time\n",
    "                self.stats['total_api_time'] += elapsed\n",
    "                self.stats['total_calls'] += 1\n",
    "                \n",
    "                if response.status == 200:\n",
    "                    result = await response.json()\n",
    "                    \n",
    "                    # Cache the result\n",
    "                    if self.use_cache:\n",
    "                        self.cache[cache_key] = result\n",
    "                    \n",
    "                    return result\n",
    "                else:\n",
    "                    error_text = await response.text()\n",
    "                    raise Exception(f\"API returned status {response.status}: {error_text}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Retry logic\n",
    "            if retry_count < self.api_config['max_retries']:\n",
    "                await asyncio.sleep(self.api_config['retry_delay'] * (2 ** retry_count))\n",
    "                return await self._call_api_async(\n",
    "                    session, context, model_output, ground_truth, retry_count + 1\n",
    "                )\n",
    "            else:\n",
    "                self.stats['api_errors'] += 1\n",
    "                # Return default low scores\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"evaluations\": {metric: {\"score\": 1} for metric in self.metrics},\n",
    "                    \"weighted_reward\": 0.0\n",
    "                }\n",
    "    \n",
    "    async def _evaluate_batch_async(\n",
    "        self,\n",
    "        contexts: List[Dict],\n",
    "        generated: List[str],\n",
    "        references: List[str]\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Evaluate a batch of generations using async API calls.\"\"\"\n",
    "        semaphore = asyncio.Semaphore(self.api_config['max_concurrent_requests'])\n",
    "        \n",
    "        async def evaluate_single(ctx, gen, ref):\n",
    "            async with semaphore:\n",
    "                async with aiohttp.ClientSession() as session:\n",
    "                    return await self._call_api_async(session, ctx, gen, ref)\n",
    "        \n",
    "        tasks = [\n",
    "            evaluate_single(ctx, gen, ref)\n",
    "            for ctx, gen, ref in zip(contexts, generated, references)\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_batch(\n",
    "        self,\n",
    "        contexts: List[Dict],\n",
    "        generated: List[str],\n",
    "        references: List[str]\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Synchronous wrapper for batch evaluation.\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return loop.run_until_complete(\n",
    "            self._evaluate_batch_async(contexts, generated, references)\n",
    "        )\n",
    "    \n",
    "    def compute_batch_rewards(\n",
    "        self,\n",
    "        contexts: List[Dict],\n",
    "        generated: List[str],\n",
    "        references: List[str]\n",
    "    ) -> Tuple[List[float], List[Dict]]:\n",
    "        \"\"\"Compute rewards for a batch of generations.\"\"\"\n",
    "        results = self.evaluate_batch(contexts, generated, references)\n",
    "        \n",
    "        rewards = []\n",
    "        all_scores = []\n",
    "        \n",
    "        for result in results:\n",
    "            if result.get(\"success\", False):\n",
    "                reward = result.get(\"weighted_reward\", 0.0)\n",
    "                scores = result.get(\"evaluations\", {})\n",
    "            else:\n",
    "                reward = 0.0\n",
    "                scores = {}\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            all_scores.append(scores)\n",
    "        \n",
    "        return rewards, all_scores\n",
    "    \n",
    "    def print_stats(self):\n",
    "        \"\"\"Print cache and API usage statistics.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Reward Model Statistics\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total API calls: {self.stats['total_calls']}\")\n",
    "        print(f\"Cache hits: {self.stats['cache_hits']}\")\n",
    "        if self.stats['total_calls'] > 0:\n",
    "            cache_rate = (self.stats['cache_hits'] / (self.stats['total_calls'] + self.stats['cache_hits'])) * 100\n",
    "            print(f\"Cache hit rate: {cache_rate:.1f}%\")\n",
    "        print(f\"API errors: {self.stats['api_errors']}\")\n",
    "        if self.stats['total_calls'] > 0:\n",
    "            avg_time = self.stats['total_api_time'] / self.stats['total_calls']\n",
    "            print(f\"Average API call time: {avg_time:.2f}s\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "# Initialize reward model\n",
    "metrics_to_use = PRIORITY_METRICS if GRPO_CONFIG[\"use_priority_metrics\"] else ALL_METRICS\n",
    "\n",
    "reward_model = APIIntegratedRewardModel(\n",
    "    api_base_url=API_BASE_URL,\n",
    "    metrics=metrics_to_use,\n",
    "    weights=REWARD_WEIGHTS,\n",
    "    api_config=API_CONFIG,\n",
    "    use_cache=API_CONFIG[\"use_cache\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Using metrics: {metrics_to_use}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af155d",
   "metadata": {},
   "source": [
    "## 6. Environment Setup\n",
    "\n",
    "Before running the training, you need to set up your environment variables for authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f543ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your environment variables here\n",
    "# You can also set these in your system environment before starting Jupyter\n",
    "\n",
    "# Supabase authentication key (required)\n",
    "os.environ[\"SUPABASE_ANON_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImdkcGFub3FjZmVwdWdxa2lzcWhmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjQ3NDQyMDcsImV4cCI6MjA4MDMyMDIwN30.S3Bsm8MDqJWOFx6vVVBislrlX09sGth1EbR1lRfPdxw\"\n",
    "\n",
    "# Groq API key (required for evaluation)\n",
    "# Get from: https://console.groq.com/keys\n",
    "os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key_here\"  # Replace with your key\n",
    "\n",
    "# Hugging Face token (optional, for model uploads)\n",
    "# os.environ[\"HF_TOKEN\"] = \"your_hugging_face_token_here\"\n",
    "\n",
    "print(\"‚úÖ Environment variables set!\")\n",
    "print(f\"üìç API URL: {API_BASE_URL}\")\n",
    "print(f\"üîë Supabase Key: {'Set' if os.getenv('SUPABASE_ANON_KEY') else 'Not Set'}\")\n",
    "print(f\"ü§ñ Groq Key: {'Set' if os.getenv('GROQ_API_KEY') else 'Not Set'}\")\n",
    "print(f\"ü§ó HF Token: {'Set' if os.getenv('HF_TOKEN') else 'Not Set'}\")\n",
    "\n",
    "# Test API connectivity\n",
    "try:\n",
    "    import aiohttp\n",
    "    import asyncio\n",
    "    \n",
    "    async def test_api():\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            supabase_key = os.getenv(\"SUPABASE_ANON_KEY\")\n",
    "            if supabase_key:\n",
    "                headers[\"Authorization\"] = f\"Bearer {supabase_key}\"\n",
    "                headers[\"apikey\"] = supabase_key\n",
    "            \n",
    "            async with session.post(API_BASE_URL, json={\"test\": \"connection\"}, headers=headers) as response:\n",
    "                return response.status\n",
    "    \n",
    "    status = asyncio.run(test_api())\n",
    "    if status == 200:\n",
    "        print(\"‚úÖ API connection successful!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  API returned status {status} (may be expected for test payload)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå API connection failed: {e}\")\n",
    "    print(\"Make sure your environment variables are set correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43cf07",
   "metadata": {},
   "source": [
    "## 7. GRPO Training\n",
    "\n",
    "Now you can run the complete GRPO training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4409c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO Trainer class (complete implementation)\n",
    "class GRPOTrainer:\n",
    "    \"\"\"GRPO Trainer for AMR prescription validation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, reward_model, config, output_dir):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.reward_model = reward_model\n",
    "        self.config = config\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config[\"learning_rate\"]\n",
    "        )\n",
    "        \n",
    "        self.metrics = {\n",
    "            \"epoch\": [],\n",
    "            \"step\": [],\n",
    "            \"loss\": [],\n",
    "            \"mean_reward\": [],\n",
    "            \"max_reward\": [],\n",
    "            \"detailed_scores\": [],\n",
    "        }\n",
    "        \n",
    "        self.global_step = 0\n",
    "        self.eval_frequency = config.get(\"eval_frequency\", 10)\n",
    "        \n",
    "        print(f\"‚úÖ GRPO Trainer initialized\")\n",
    "        print(f\"   Eval frequency: Every {self.eval_frequency} steps\")\n",
    "    \n",
    "    def generate_responses(self, prompts: List[str], num_generations: int) -> List[List[str]]:\n",
    "        \"\"\"Generate multiple responses for each prompt.\"\"\"\n",
    "        self.model.eval()\n",
    "        all_generations = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in prompts:\n",
    "                formatted_prompt = f\"<|user|>\\\\n{prompt}\\\\n<|assistant|>\\\\n\"\n",
    "                \n",
    "                inputs = self.tokenizer(\n",
    "                    formatted_prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                ).to(self.model.device)\n",
    "                \n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.config[\"max_length\"],\n",
    "                    num_return_sequences=num_generations,\n",
    "                    temperature=self.config[\"temperature\"],\n",
    "                    top_p=self.config[\"top_p\"],\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "                \n",
    "                generations = []\n",
    "                for output in outputs:\n",
    "                    decoded = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "                    if \"<|assistant|>\" in decoded:\n",
    "                        decoded = decoded.split(\"<|assistant|>\")[-1].strip()\n",
    "                    generations.append(decoded)\n",
    "                \n",
    "                all_generations.append(generations)\n",
    "        \n",
    "        return all_generations\n",
    "    \n",
    "    def compute_advantages(self, rewards: List[float]) -> List[float]:\n",
    "        \"\"\"Compute group-relative advantages.\"\"\"\n",
    "        rewards = np.array(rewards)\n",
    "        mean_reward = np.mean(rewards)\n",
    "        std_reward = np.std(rewards) + 1e-8\n",
    "        advantages = (rewards - mean_reward) / std_reward\n",
    "        return advantages.tolist()\n",
    "    \n",
    "    def compute_loss(self, prompts: List[str], generations: List[str], advantages: List[float]):\n",
    "        \"\"\"Compute the GRPO loss.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for prompt, generation, advantage in zip(prompts, generations, advantages):\n",
    "            formatted_text = f\"<|user|>\\\\n{prompt}\\\\n<|assistant|>\\\\n{generation}\"\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                formatted_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            outputs = self.model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            weighted_loss = outputs.loss * advantage\n",
    "            total_loss += weighted_loss\n",
    "        \n",
    "        return total_loss / len(prompts)\n",
    "    \n",
    "    def train_step(self, batch_prompts: List[str], batch_references: List[str], \n",
    "                   batch_contexts: List[Dict], evaluate_this_step: bool = False):\n",
    "        \"\"\"Single training step for GRPO.\"\"\"\n",
    "        self.global_step += 1\n",
    "        \n",
    "        # Generate responses\n",
    "        all_generations = self.generate_responses(\n",
    "            batch_prompts,\n",
    "            self.config[\"num_generations_per_prompt\"]\n",
    "        )\n",
    "        \n",
    "        # Flatten\n",
    "        flat_prompts = []\n",
    "        flat_generations = []\n",
    "        flat_references = []\n",
    "        flat_contexts = []\n",
    "        \n",
    "        for prompt, generations, reference, context in zip(\n",
    "            batch_prompts, all_generations, batch_references, batch_contexts\n",
    "        ):\n",
    "            for generation in generations:\n",
    "                flat_prompts.append(prompt)\n",
    "                flat_generations.append(generation)\n",
    "                flat_references.append(reference)\n",
    "                flat_contexts.append(context)\n",
    "        \n",
    "        # Compute rewards\n",
    "        all_rewards = []\n",
    "        all_scores = None\n",
    "        \n",
    "        if evaluate_this_step:\n",
    "            all_rewards, all_scores = self.reward_model.compute_batch_rewards(\n",
    "                flat_contexts, flat_generations, flat_references\n",
    "            )\n",
    "        else:\n",
    "            # Simple heuristic rewards\n",
    "            for gen, ref in zip(flat_generations, flat_references):\n",
    "                len_ratio = len(gen.split()) / max(len(ref.split()), 1)\n",
    "                reward = 1.0 - abs(1.0 - len_ratio)\n",
    "                reward = max(0, min(1, reward)) * 0.5\n",
    "                all_rewards.append(reward)\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages = self.compute_advantages(all_rewards)\n",
    "        \n",
    "        # Update model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.compute_loss(flat_prompts, flat_generations, advantages)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"mean_reward\": np.mean(all_rewards),\n",
    "            \"max_reward\": np.max(all_rewards),\n",
    "            \"min_reward\": np.min(all_rewards),\n",
    "            \"detailed_scores\": all_scores,\n",
    "            \"evaluated\": evaluate_this_step\n",
    "        }\n",
    "    \n",
    "    def train(self, dataset, num_epochs):\n",
    "        \"\"\"Main training loop.\"\"\"\n",
    "        print(f\"\\\\n{'='*80}\")\n",
    "        print(f\"üöÄ Starting GRPO Training for AMR\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"üìä Epochs: {num_epochs}\")\n",
    "        print(f\"üì¶ Dataset size: {len(dataset)}\")\n",
    "        print(f\"üî¢ Batch size: {self.config['batch_size']}\")\n",
    "        print(f\"‚è±Ô∏è  Eval frequency: Every {self.eval_frequency} steps\")\n",
    "        print(f\"üé≤ Generations per prompt: {self.config['num_generations_per_prompt']}\")\n",
    "        print(f\"{'='*80}\\\\n\")\n",
    "        \n",
    "        epoch_pbar = tqdm(range(num_epochs), desc=\"üìö Epochs\", position=0, leave=True)\n",
    "        \n",
    "        for epoch in epoch_pbar:\n",
    "            epoch_pbar.set_description(f\"üìö Epoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            epoch_metrics = {\n",
    "                \"loss\": [],\n",
    "                \"mean_reward\": [],\n",
    "                \"max_reward\": [],\n",
    "            }\n",
    "            \n",
    "            num_batches = (len(dataset) + self.config[\"batch_size\"] - 1) // self.config[\"batch_size\"]\n",
    "            \n",
    "            step_pbar = tqdm(\n",
    "                range(0, len(dataset), self.config[\"batch_size\"]),\n",
    "                desc=f\"  üîÑ Steps\",\n",
    "                position=1,\n",
    "                leave=False,\n",
    "                total=num_batches\n",
    "            )\n",
    "            \n",
    "            for i in step_pbar:\n",
    "                batch = dataset[i:i + self.config[\"batch_size\"]]\n",
    "                batch_prompts = batch[\"prompt\"]\n",
    "                batch_references = batch[\"reference\"]\n",
    "                batch_contexts = batch[\"context\"]\n",
    "                \n",
    "                evaluate_this_step = (self.global_step % self.eval_frequency == 0)\n",
    "                \n",
    "                metrics = self.train_step(batch_prompts, batch_references, batch_contexts, evaluate_this_step)\n",
    "                \n",
    "                for key in epoch_metrics:\n",
    "                    if key in metrics:\n",
    "                        epoch_metrics[key].append(metrics[key])\n",
    "                \n",
    "                eval_marker = \"üìä EVAL\" if evaluate_this_step else \"‚ö° FAST\"\n",
    "                step_pbar.set_postfix({\n",
    "                    'type': eval_marker,\n",
    "                    'loss': f\"{metrics['loss']:.4f}\",\n",
    "                    'reward': f\"{metrics['mean_reward']:.4f}\",\n",
    "                    'max': f\"{metrics['max_reward']:.4f}\",\n",
    "                    'global': self.global_step\n",
    "                })\n",
    "                \n",
    "                if metrics.get('detailed_scores'):\n",
    "                    self.metrics['detailed_scores'].append({\n",
    "                        'step': self.global_step,\n",
    "                        'scores': metrics['detailed_scores']\n",
    "                    })\n",
    "            \n",
    "            step_pbar.close()\n",
    "            \n",
    "            # Epoch summary\n",
    "            epoch_loss = np.mean(epoch_metrics[\"loss\"])\n",
    "            epoch_mean_reward = np.mean(epoch_metrics[\"mean_reward\"])\n",
    "            epoch_max_reward = np.mean(epoch_metrics[\"max_reward\"])\n",
    "            \n",
    "            epoch_pbar.set_postfix({\n",
    "                'loss': f\"{epoch_loss:.4f}\",\n",
    "                'reward': f\"{epoch_mean_reward:.4f}\",\n",
    "                'max': f\"{epoch_max_reward:.4f}\"\n",
    "            })\n",
    "            \n",
    "            self.metrics[\"epoch\"].append(epoch + 1)\n",
    "            self.metrics[\"loss\"].append(epoch_loss)\n",
    "            self.metrics[\"mean_reward\"].append(epoch_mean_reward)\n",
    "            self.metrics[\"max_reward\"].append(epoch_max_reward)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_dir = f\"{self.output_dir}/checkpoint-epoch-{epoch + 1}\"\n",
    "            self.save_checkpoint(checkpoint_dir)\n",
    "            \n",
    "            print(f\"\\\\nüìä Epoch {epoch + 1} Summary: Loss={epoch_loss:.4f}, Reward={epoch_mean_reward:.4f}, Max={epoch_max_reward:.4f}\")\n",
    "        \n",
    "        epoch_pbar.close()\n",
    "        \n",
    "        print(f\"\\\\n{'='*80}\")\n",
    "        print(\"‚úÖ Training Complete!\")\n",
    "        print(f\"{'='*80}\\\\n\")\n",
    "        \n",
    "        self.reward_model.print_stats()\n",
    "    \n",
    "    def save_checkpoint(self, checkpoint_dir):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(checkpoint_dir)\n",
    "        self.tokenizer.save_pretrained(checkpoint_dir)\n",
    "        \n",
    "        with open(f\"{checkpoint_dir}/metrics.json\", \"w\") as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ GRPO Trainer class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    reward_model=reward_model,\n",
    "    config=GRPO_CONFIG,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a7ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train(\n",
    "    dataset=train_dataset,\n",
    "    num_epochs=GRPO_CONFIG[\"num_train_epochs\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f071a",
   "metadata": {},
   "source": [
    "## 8. Save Final Model\n",
    "\n",
    "After training, save the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba74dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_dir = f\"{OUTPUT_DIR}/final_model\"\n",
    "os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "print(f\"‚úÖ Final model saved to {final_model_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
