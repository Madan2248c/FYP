{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Finetuning for Style Transfer\n",
    "\n",
    "This notebook implements Group Relative Policy Optimization (GRPO) for fine-tuning language models on style transfer tasks.\n",
    "\n",
    "GRPO is a reinforcement learning method that optimizes language models by:\n",
    "- Generating multiple outputs per prompt\n",
    "- Computing rewards for each output\n",
    "- Using group-relative advantages for stable optimization\n",
    "- Updating the model to favor high-reward outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Install required packages\n",
    "# %%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "%uv pip install transformers==4.55.4\n",
    "%uv pip install --no-deps trl==0.22.2\n",
    "%uv pip install nltk -q\n",
    "\n",
    "%uv pip install -q aiohttp nest-asyncio\n"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: unsloth in /usr/local/lib/python3.12/site-packages (2025.10.4)\r\n",
      "Requirement already satisfied: unsloth_zoo>=2025.10.4 in /usr/local/lib/python3.12/site-packages (from unsloth) (2025.10.4)\r\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/site-packages (from unsloth) (0.45.1)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from unsloth) (25.0)\r\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/site-packages (from unsloth) (2.8.0+cu129)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/site-packages (from unsloth) (0.23.0+cu129)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from unsloth) (2.1.2)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from unsloth) (4.67.1)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/site-packages (from unsloth) (7.0.0)\r\n",
      "Requirement already satisfied: tyro in /usr/local/lib/python3.12/site-packages (from unsloth) (0.9.35)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/site-packages (from unsloth) (5.29.2)\r\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.12/site-packages (from unsloth) (0.0.32.post2)\r\n",
      "Requirement already satisfied: bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 in /usr/local/lib/python3.12/site-packages (from unsloth) (0.48.1)\r\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/site-packages (from unsloth) (3.4.0)\r\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/site-packages (from unsloth) (0.2.1)\r\n",
      "Requirement already satisfied: datasets!=4.0.*,!=4.1.0,>=3.4.1 in /usr/local/lib/python3.12/site-packages (from unsloth) (4.2.0)\r\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/site-packages (from unsloth) (1.10.1)\r\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/site-packages (from unsloth) (0.17.1)\r\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/site-packages (from unsloth) (0.34.4)\r\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/site-packages (from unsloth) (0.1.9)\r\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/site-packages (from unsloth) (0.35.1)\r\n",
      "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3 in /usr/local/lib/python3.12/site-packages (from unsloth) (4.55.4)\r\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.23.0,>=0.7.9 in /usr/local/lib/python3.12/site-packages (from unsloth) (0.22.2)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.13.1)\r\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (21.0.0)\r\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.4.0)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.3.2)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.32.5)\r\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.28.1)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.6.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2024.6.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/site-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface_hub>=0.34.0->unsloth) (1.1.9)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (70.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (3.1.4)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.9.86)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.9.79)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.9.79)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.9.1.4 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.9.1.4)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.4.1.4 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (11.4.1.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.10.19 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (10.3.10.19)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.5.82 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (11.7.5.82)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.10.65 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.5.10.65)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.9.79)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.9.86)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.14.1.1 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (1.14.1.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/site-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3->unsloth) (2025.9.1)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/site-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3->unsloth) (0.21.4)\r\n",
      "Requirement already satisfied: torchao in /usr/local/lib/python3.12/site-packages (from unsloth_zoo>=2025.10.4->unsloth) (0.14.0)\r\n",
      "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.12/site-packages (from unsloth_zoo>=2025.10.4->unsloth) (25.1.1)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/site-packages (from unsloth_zoo>=2025.10.4->unsloth) (11.0.0)\r\n",
      "Requirement already satisfied: msgspec in /usr/local/lib/python3.12/site-packages (from unsloth_zoo>=2025.10.4->unsloth) (0.19.0)\r\n",
      "Requirement already satisfied: mistral_common in /usr/local/lib/python3.12/site-packages (from unsloth_zoo>=2025.10.4->unsloth) (1.8.5)\r\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/site-packages (from diffusers->unsloth) (8.7.0)\r\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/site-packages (from tyro->unsloth) (0.17.0)\r\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/site-packages (from tyro->unsloth) (14.1.0)\r\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.12/site-packages (from tyro->unsloth) (1.7.2)\r\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/site-packages (from tyro->unsloth) (4.4.4)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.10.8)\r\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (4.10.0)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2024.8.30)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.0.9)\r\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.10)\r\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.16.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.4.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.5.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/site-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch>=2.4.0->unsloth) (2.1.5)\r\n",
      "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/site-packages (from mistral_common->unsloth_zoo>=2025.10.4->unsloth) (2.11.7)\r\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/site-packages (from mistral_common->unsloth_zoo>=2025.10.4->unsloth) (4.25.1)\r\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/site-packages (from mistral_common->unsloth_zoo>=2025.10.4->unsloth) (0.12.0)\r\n",
      "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.12/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common->unsloth_zoo>=2025.10.4->unsloth) (2.10.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.4.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (24.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (6.1.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.13.1)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common->unsloth_zoo>=2025.10.4->unsloth) (2025.4.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common->unsloth_zoo>=2025.10.4->unsloth) (0.36.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common->unsloth_zoo>=2025.10.4->unsloth) (0.27.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3.0,>=2.7->mistral_common->unsloth_zoo>=2025.10.4->unsloth) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/site-packages (from pydantic<3.0,>=2.7->mistral_common->unsloth_zoo>=2025.10.4->unsloth) (2.33.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3.0,>=2.7->mistral_common->unsloth_zoo>=2025.10.4->unsloth) (0.4.1)\r\n",
      "Requirement already satisfied: pycountry>=23 in /usr/local/lib/python3.12/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common->unsloth_zoo>=2025.10.4->unsloth) (24.6.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.17.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.3.1)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 7ms\u001b[0m\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "from tqdm.auto import tqdm\n"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import warnings\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from functools import lru_cache\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"\u2705 Unsloth imported successfully!\")\n"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu129 for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n",
      "PyTorch version: 2.8.0+cu129\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100-SXM4-40GB\n",
      "\u2705 Unsloth imported successfully!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# ============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "OUTPUT_DIR = \"./grpo_style_transfer_model\"\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION API CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# IMPORTANT: Set your evaluation API endpoint here\n",
    "API_BASE_URL = \"https://qtt14d7li0.execute-api.us-west-2.amazonaws.com/api/\"  # UPDATE THIS!\n",
    "\n",
    "# Priority metrics for training (faster, focused on key aspects)\n",
    "PRIORITY_METRICS = ['style_similarity', 'rephrase_accuracy']\n",
    "\n",
    "# All metrics for comprehensive evaluation\n",
    "ALL_METRICS = [\n",
    "    'style_similarity',\n",
    "    'meaning_preservation',\n",
    "    'coherence',\n",
    "    'fluency',\n",
    "    'content_length',\n",
    "    'rephrase_accuracy'\n",
    "]\n",
    "\n",
    "# Reward weights for each metric (must sum to 1.0)\n",
    "REWARD_WEIGHTS = {\n",
    "    'style_similarity': 0.30,      # Priority metric\n",
    "    'rephrase_accuracy': 0.30,     # Priority metric\n",
    "    'meaning_preservation': 0.15,\n",
    "    'coherence': 0.10,\n",
    "    'fluency': 0.10,\n",
    "    'content_length': 0.05\n",
    "}\n",
    "\n",
    "# API Configuration\n",
    "API_CONFIG = {\n",
    "    \"max_concurrent_requests\": 100,\n",
    "    \"timeout_seconds\": 30,\n",
    "    \"max_retries\": 3,\n",
    "    \"retry_delay\": 1.0,  # seconds\n",
    "    \"use_cache\": True,\n",
    "}\n",
    "\n",
    "# Evaluation frequency (evaluate every N training steps to reduce API calls)\n",
    "EVAL_FREQUENCY = 5  # Evaluate every 5 steps\n",
    "\n",
    "# ============================================================================\n",
    "# GRPO HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "GRPO_CONFIG = {\n",
    "    \"num_generations_per_prompt\": 2,  # Number of outputs to generate per prompt\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"max_length\": 512,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"kl_coef\": 0.05,  # KL divergence coefficient\n",
    "    \"clip_range\": 0.2,  # PPO clipping parameter\n",
    "    \"vf_coef\": 0.1,  # Value function coefficient\n",
    "    \"eval_frequency\": EVAL_FREQUENCY,  # Evaluate every N steps\n",
    "    \"use_priority_metrics\": False,  # Use ALL 6 metrics during training for comprehensive evaluation\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# UNSLOTH CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Unsloth handles LoRA and quantization automatically with optimal settings\n",
    "MAX_SEQ_LENGTH = 2048  # Choose any! Unsloth auto-supports RoPE scaling\n",
    "LOAD_IN_4BIT = True  # Use 4-bit quantization for memory efficiency\n",
    "\n",
    "# LoRA Configuration (Unsloth optimized)\n",
    "LORA_R = 16  # LoRA rank\n",
    "LORA_ALPHA = 32  # LoRA alpha\n",
    "LORA_DROPOUT = 0  # Unsloth recommends 0 for faster training\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "print(\"\u2705 Configuration loaded successfully!\")\n",
    "print(f\"\ud83d\udccd API Base URL: {API_BASE_URL}\")\n",
    "print(f\"\ud83c\udfaf Using Metrics: ALL 6 METRICS (comprehensive evaluation)\")\n",
    "print(f\"   - {', '.join(ALL_METRICS)}\")\n",
    "print(f\"\ud83d\udcca Evaluation Frequency: Every {EVAL_FREQUENCY} steps\")\n",
    "print(f\"\u26a1 Max Concurrent API Requests: {API_CONFIG['max_concurrent_requests']}\")\n",
    "print(f\"\ud83d\udca1 Note: Using all 6 metrics = 3x more API calls than priority mode\")\n"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Configuration loaded successfully!\n",
      "\ud83d\udccd API Base URL: https://qtt14d7li0.execute-api.us-west-2.amazonaws.com/api/\n",
      "\ud83c\udfaf Using Metrics: ALL 6 METRICS (comprehensive evaluation)\n",
      "   - style_similarity, meaning_preservation, coherence, fluency, content_length, rephrase_accuracy\n",
      "\ud83d\udcca Evaluation Frequency: Every 5 steps\n",
      "\u26a1 Max Concurrent API Requests: 100\n",
      "\ud83d\udca1 Note: Using all 6 metrics = 3x more API calls than priority mode\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Prompt template for style transfer\n",
    "STYLE_TRANSFER_PROMPT = \"\"\"**Role:** You are an expert content writer for SurveySparrow platform with 20+ years of experience in writing blog content for SaaS platform.\n",
    "\n",
    "**Task**\n",
    "Your task is to rewrite the provided text to match the SurveySparrow writing style exactly without any hallucination or leaving any content from the input blog content. also make sure that you read the content line-by-line so that the meaning doesn't change for each line.\n",
    "\n",
    "**Rules:**\n",
    "1. You MUST preserve the core meaning, all facts, and key entities from the original text.\n",
    "2. Do NOT add any new information, opinions, or details.\n",
    "3. Do NOT omit any critical information from the original.\n",
    "4. Adopt the SurveySparrow writing style in terms of tone, vocabulary, and sentence structure.\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Output:\n",
    "\"\"\"\n",
    "\n",
    "def load_style_transfer_dataset_from_hf(dataset_name: str, split: str = \"train\") -> Dataset:\n",
    "    \"\"\"\n",
    "    Load the style transfer dataset from Hugging Face.\n",
    "\n",
    "    Dataset columns:\n",
    "    - rewritten_text_output: Neutral style content (INPUT for model)\n",
    "    - original_text_input: SurveySparrow style content (OUTPUT/target for model)\n",
    "\n",
    "    Task: Convert neutral content \u2192 SurveySparrow style\n",
    "    \"\"\"\n",
    "    print(f\"\ud83d\udce5 Loading dataset from Hugging Face: {dataset_name} ({split} split)\")\n",
    "\n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "\n",
    "    print(f\"\u2705 Loaded {len(dataset)} examples\")\n",
    "    print(f\"\ud83d\udccb Dataset columns: {dataset.column_names}\")\n",
    "\n",
    "    # Process dataset to create prompt and reference pairs\n",
    "    processed_data = []\n",
    "\n",
    "    for idx, item in enumerate(dataset):\n",
    "        # Get the columns\n",
    "        neutral_text = item.get('rewritten_text_output', '')  # Input (neutral style)\n",
    "        surveysparrow_text = item.get('original_text_input', '')  # Output (SurveySparrow style)\n",
    "\n",
    "        # Skip if either is empty\n",
    "        if not neutral_text or not surveysparrow_text:\n",
    "            print(f\"\u26a0\ufe0f  Skipping example {idx}: missing text\")\n",
    "            continue\n",
    "\n",
    "        # Format the prompt using the template\n",
    "        formatted_prompt = STYLE_TRANSFER_PROMPT.format(input_text=neutral_text.strip())\n",
    "\n",
    "        processed_data.append({\n",
    "            \"prompt\": formatted_prompt,\n",
    "            \"reference\": surveysparrow_text.strip(),\n",
    "            \"input_text\": neutral_text.strip(),  # Store for evaluation API\n",
    "        })\n",
    "\n",
    "    # Convert to Dataset\n",
    "    processed_dataset = Dataset.from_list(processed_data)\n",
    "\n",
    "    print(f\"\u2705 Processed {len(processed_dataset)} examples successfully\")\n",
    "\n",
    "    return processed_dataset\n",
    "\n",
    "# Load your dataset from Hugging Face\n",
    "train_dataset = load_style_transfer_dataset_from_hf(\n",
    "    dataset_name=\"madan2248c/styletrasfer_final\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Dataset Example:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n\ud83d\udcdd Prompt (first 500 chars):\")\n",
    "print(train_dataset[0][\"prompt\"][:500] + \"...\")\n",
    "print(f\"\\n\u2728 Reference Output (first 300 chars):\")\n",
    "print(train_dataset[0][\"reference\"][:300] + \"...\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udce5 Loading dataset from Hugging Face: madan2248c/styletrasfer_final (train split)\n",
      "\u2705 Loaded 9646 examples\n",
      "\ud83d\udccb Dataset columns: ['original_text_input', 'rewritten_text_output']\n",
      "\u2705 Processed 9646 examples successfully\n",
      "\n",
      "================================================================================\n",
      "Dataset Example:\n",
      "================================================================================\n",
      "\n",
      "\ud83d\udcdd Prompt (first 500 chars):\n",
      "**Role:** You are an expert content writer for SurveySparrow platform with 20+ years of experience in writing blog content for SaaS platform.\n",
      "\n",
      "**Task**\n",
      "Your task is to rewrite the provided text to match the SurveySparrow writing style exactly without any hallucination or leaving any content from the input blog content. also make sure that you read the content line-by-line so that the meaning doesn't change for each line.\n",
      "\n",
      "**Rules:**\n",
      "1. You MUST preserve the core meaning, all facts, and key entit...\n",
      "\n",
      "\u2728 Reference Output (first 300 chars):\n",
      "Ready for production Once the prototyping stage is over, the design is handed over to the developers for coding. During this stage, the designer should clearly communicate how each portion of the design looks and works. In fact, this is a major part of the process for which you need to spend a sizea...\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Load model and tokenizer with Unsloth (optimized for speed and memory)\n",
    "print(f\"Loading model with Unsloth: {MODEL_NAME}\")\n",
    "print(f\"This may take a few minutes on first run...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect dtype (None = auto)\n",
    "    load_in_4bit=LOAD_IN_4BIT,  # Use 4-bit quantization\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Model and tokenizer loaded successfully!\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"   4-bit quantization: {LOAD_IN_4BIT}\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading model with Unsloth: meta-llama/Llama-3.1-8B-Instruct\n",
      "This may take a few minutes on first run...\n",
      "==((====))==  Unsloth 2025.10.4: Fast Llama patching. Transformers: 4.55.4.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 8.0. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\u2705 Model and tokenizer loaded successfully!\n",
      "   Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "   Max sequence length: 2048\n",
      "   4-bit quantization: True\n",
      "   Vocab size: 128256\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Add LoRA adapters with Unsloth (super fast!)\n",
    "print(\"Adding LoRA adapters with Unsloth optimization...\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,  # LoRA rank\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",  # Supports: \"none\", \"all\", \"lora_only\"\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # Rank stabilized LoRA\n",
    "    loftq_config=None,  # LoftQ quantization\n",
    ")\n",
    "\n",
    "print(\"\u2705 LoRA adapters added successfully!\")\n",
    "print(\"\\n\ud83d\udcca Trainable Parameters:\")\n",
    "model.print_trainable_parameters()\n"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Adding LoRA adapters with Unsloth optimization...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth 2025.10.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 LoRA adapters added successfully!\n",
      "\n",
      "\ud83d\udcca Trainable Parameters:\n",
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. API-Integrated Reward Function\n",
    "\n",
    "This reward model integrates with your external LLM-as-a-Judge evaluation API to compute rewards based on:\n",
    "- **Style Similarity** (30% weight) - Priority metric\n",
    "- **Rephrase Accuracy** (30% weight) - Priority metric  \n",
    "- **Meaning Preservation** (15% weight)\n",
    "- **Coherence** (10% weight)\n",
    "- **Fluency** (10% weight)\n",
    "- **Content Length** (5% weight)\n",
    "\n",
    "Features:\n",
    "- \u2705 Async batching for 100 concurrent API requests\n",
    "- \u2705 Response caching to avoid duplicate calls\n",
    "- \u2705 Retry logic with exponential backoff\n",
    "- \u2705 Priority metrics mode for faster training\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "from typing import Any\n",
    "\n",
    "class APIIntegratedRewardModel:\n",
    "    \"\"\"\n",
    "    Reward model that integrates with external LLM-as-a-Judge evaluation API.\n",
    "\n",
    "    Features:\n",
    "    - Async batch processing with concurrency control\n",
    "    - Response caching to avoid duplicate API calls\n",
    "    - Retry logic with exponential backoff\n",
    "    - Priority metrics mode for faster training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_base_url: str,\n",
    "        metrics: List[str],\n",
    "        weights: Dict[str, float],\n",
    "        api_config: Dict[str, Any],\n",
    "        use_cache: bool = True\n",
    "    ):\n",
    "        self.api_base_url = api_base_url\n",
    "        self.metrics = metrics\n",
    "        self.weights = weights\n",
    "        self.api_config = api_config\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        # Cache for API responses: key = hash(input_text, model_output, ground_truth, metric)\n",
    "        self.cache = {} if use_cache else None\n",
    "\n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_calls': 0,\n",
    "            'cache_hits': 0,\n",
    "            'api_errors': 0,\n",
    "            'total_api_time': 0.0\n",
    "        }\n",
    "\n",
    "        print(f\"\u2705 Reward Model initialized with {len(metrics)} metrics\")\n",
    "        print(f\"   Metrics: {metrics}\")\n",
    "        print(f\"   Caching: {'Enabled' if use_cache else 'Disabled'}\")\n",
    "\n",
    "    def _create_cache_key(self, input_text: str, model_output: str, ground_truth: str, metric: str) -> str:\n",
    "        \"\"\"Create a unique cache key for an API call.\"\"\"\n",
    "        content = f\"{input_text}|{model_output}|{ground_truth}|{metric}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "\n",
    "    async def _call_api_single(\n",
    "        self,\n",
    "        session: aiohttp.ClientSession,\n",
    "        metric: str,\n",
    "        input_text: str,\n",
    "        model_output: str,\n",
    "        ground_truth: str,\n",
    "        retry_count: int = 0\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Make a single API call to evaluate one metric.\n",
    "        Returns the API response with score and justification.\n",
    "        \"\"\"\n",
    "        # Check cache first\n",
    "        if self.use_cache:\n",
    "            cache_key = self._create_cache_key(input_text, model_output, ground_truth, metric)\n",
    "            if cache_key in self.cache:\n",
    "                self.stats['cache_hits'] += 1\n",
    "                return self.cache[cache_key]\n",
    "\n",
    "        # Prepare API request\n",
    "        url = f\"{self.api_base_url}/evaluate?metric={metric}\"\n",
    "        payload = {\n",
    "            \"input_text\": input_text,\n",
    "            \"model_output\": model_output,\n",
    "            \"ground_truth\": ground_truth\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            async with session.post(\n",
    "                url,\n",
    "                json=payload,\n",
    "                timeout=aiohttp.ClientTimeout(total=self.api_config['timeout_seconds'])\n",
    "            ) as response:\n",
    "                elapsed = time.time() - start_time\n",
    "                self.stats['total_api_time'] += elapsed\n",
    "                self.stats['total_calls'] += 1\n",
    "\n",
    "                if response.status == 200:\n",
    "                    result = await response.json()\n",
    "\n",
    "                    # Cache the result\n",
    "                    if self.use_cache:\n",
    "                        self.cache[cache_key] = result\n",
    "\n",
    "                    return result\n",
    "                else:\n",
    "                    error_text = await response.text()\n",
    "                    raise Exception(f\"API returned status {response.status}: {error_text}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Retry logic\n",
    "            if retry_count < self.api_config['max_retries']:\n",
    "                await asyncio.sleep(self.api_config['retry_delay'] * (2 ** retry_count))\n",
    "                return await self._call_api_single(\n",
    "                    session, metric, input_text, model_output, ground_truth, retry_count + 1\n",
    "                )\n",
    "            else:\n",
    "                self.stats['api_errors'] += 1\n",
    "                print(f\"\u26a0\ufe0f  API call failed for metric '{metric}' after {retry_count} retries: {str(e)}\")\n",
    "                # Return a default low score on failure\n",
    "                return {\n",
    "                    \"metric\": metric,\n",
    "                    \"evaluation\": {\n",
    "                        metric: 1,  # Low score (1 out of 5)\n",
    "                        \"justification\": f\"API call failed: {str(e)}\"\n",
    "                    }\n",
    "                }\n",
    "\n",
    "    async def _evaluate_batch_async(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        generated: List[str],\n",
    "        references: List[str]\n",
    "    ) -> List[Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Evaluate a batch of generations using async API calls.\n",
    "        Returns list of metric scores for each generation.\n",
    "        \"\"\"\n",
    "        # Create semaphore to limit concurrent requests\n",
    "        semaphore = asyncio.Semaphore(self.api_config['max_concurrent_requests'])\n",
    "\n",
    "        async def evaluate_single(prompt, gen, ref):\n",
    "            async with semaphore:\n",
    "                async with aiohttp.ClientSession() as session:\n",
    "                    # Call API for each metric\n",
    "                    tasks = [\n",
    "                        self._call_api_single(session, metric, prompt, gen, ref)\n",
    "                        for metric in self.metrics\n",
    "                    ]\n",
    "                    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "                    # Extract scores from results\n",
    "                    scores = {}\n",
    "                    for result in results:\n",
    "                        if isinstance(result, Exception):\n",
    "                            continue\n",
    "                        if 'evaluation' in result:\n",
    "                            metric_name = result['metric']\n",
    "                            eval_data = result['evaluation']\n",
    "                            if metric_name in eval_data:\n",
    "                                scores[metric_name] = eval_data[metric_name]\n",
    "\n",
    "                    return scores\n",
    "\n",
    "        # Process all generations in parallel\n",
    "        tasks = [\n",
    "            evaluate_single(p, g, r)\n",
    "            for p, g, r in zip(prompts, generated, references)\n",
    "        ]\n",
    "        all_scores = await asyncio.gather(*tasks)\n",
    "\n",
    "        return all_scores\n",
    "\n",
    "    def evaluate_batch(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        generated: List[str],\n",
    "        references: List[str]\n",
    "    ) -> List[Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Synchronous wrapper for batch evaluation.\n",
    "        Returns list of metric scores for each generation.\n",
    "        \"\"\"\n",
    "        # Run async function in event loop\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return loop.run_until_complete(\n",
    "            self._evaluate_batch_async(prompts, generated, references)\n",
    "        )\n",
    "\n",
    "    def compute_reward(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        generated: str,\n",
    "        reference: str,\n",
    "        scores: Dict[str, float] = None\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute weighted reward from metric scores.\n",
    "\n",
    "        Args:\n",
    "            prompt: Input prompt\n",
    "            generated: Generated text\n",
    "            reference: Ground truth reference\n",
    "            scores: Pre-computed metric scores (if None, will call API)\n",
    "\n",
    "        Returns:\n",
    "            Weighted reward score normalized to [0, 1] range\n",
    "        \"\"\"\n",
    "        # Get scores from API if not provided\n",
    "        if scores is None:\n",
    "            scores_list = self.evaluate_batch([prompt], [generated], [reference])\n",
    "            scores = scores_list[0] if scores_list else {}\n",
    "\n",
    "        # Compute weighted reward\n",
    "        total_reward = 0.0\n",
    "        total_weight = 0.0\n",
    "\n",
    "        for metric in self.metrics:\n",
    "            if metric in scores and metric in self.weights:\n",
    "                # Normalize score from 1-5 scale to 0-1 scale\n",
    "                normalized_score = (scores[metric] - 1) / 4.0  # Maps 1->0, 5->1\n",
    "                total_reward += normalized_score * self.weights[metric]\n",
    "                total_weight += self.weights[metric]\n",
    "\n",
    "        # Normalize by total weight (should be 1.0 if weights are properly configured)\n",
    "        if total_weight > 0:\n",
    "            final_reward = total_reward / total_weight\n",
    "        else:\n",
    "            final_reward = 0.0\n",
    "\n",
    "        return final_reward\n",
    "\n",
    "    def compute_batch_rewards(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        generated: List[str],\n",
    "        references: List[str]\n",
    "    ) -> Tuple[List[float], List[Dict[str, float]]]:\n",
    "        \"\"\"\n",
    "        Compute rewards for a batch of generations.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (rewards, all_scores) where:\n",
    "            - rewards: List of weighted reward values [0-1]\n",
    "            - all_scores: List of dict with individual metric scores [1-5]\n",
    "        \"\"\"\n",
    "        # Get all scores via batch API call\n",
    "        all_scores = self.evaluate_batch(prompts, generated, references)\n",
    "\n",
    "        # Compute rewards from scores\n",
    "        rewards = []\n",
    "        for scores in all_scores:\n",
    "            reward = self.compute_reward(None, None, None, scores=scores)\n",
    "            rewards.append(reward)\n",
    "\n",
    "        return rewards, all_scores\n",
    "\n",
    "    def print_stats(self):\n",
    "        \"\"\"Print cache and API usage statistics.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Reward Model Statistics\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total API calls: {self.stats['total_calls']}\")\n",
    "        print(f\"Cache hits: {self.stats['cache_hits']}\")\n",
    "        if self.stats['total_calls'] > 0:\n",
    "            cache_rate = (self.stats['cache_hits'] / (self.stats['total_calls'] + self.stats['cache_hits'])) * 100\n",
    "            print(f\"Cache hit rate: {cache_rate:.1f}%\")\n",
    "        print(f\"API errors: {self.stats['api_errors']}\")\n",
    "        if self.stats['total_calls'] > 0:\n",
    "            avg_time = self.stats['total_api_time'] / self.stats['total_calls']\n",
    "            print(f\"Average API call time: {avg_time:.2f}s\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Initialize reward model\n",
    "metrics_to_use = PRIORITY_METRICS if GRPO_CONFIG[\"use_priority_metrics\"] else ALL_METRICS\n",
    "\n",
    "reward_model = APIIntegratedRewardModel(\n",
    "    api_base_url=API_BASE_URL,\n",
    "    metrics=metrics_to_use,\n",
    "    weights=REWARD_WEIGHTS,\n",
    "    api_config=API_CONFIG,\n",
    "    use_cache=API_CONFIG[\"use_cache\"]\n",
    ")\n",
    "\n",
    "print(f\"\ud83c\udfaf Using metrics: {metrics_to_use}\")"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Reward Model initialized with 6 metrics\n",
      "   Metrics: ['style_similarity', 'meaning_preservation', 'coherence', 'fluency', 'content_length', 'rephrase_accuracy']\n",
      "   Caching: Enabled\n",
      "\ud83c\udfaf Using metrics: ['style_similarity', 'meaning_preservation', 'coherence', 'fluency', 'content_length', 'rephrase_accuracy']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GRPO Training Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "class GRPOTrainer:\n",
    "    \"\"\"\n",
    "    GRPO Trainer with API-integrated rewards and optimized evaluation frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, reward_model, config, output_dir):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.reward_model = reward_model\n",
    "        self.config = config\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config[\"learning_rate\"]\n",
    "        )\n",
    "        \n",
    "        # Track metrics\n",
    "        self.metrics = {\n",
    "            \"epoch\": [],\n",
    "            \"step\": [],\n",
    "            \"loss\": [],\n",
    "            \"mean_reward\": [],\n",
    "            \"max_reward\": [],\n",
    "            \"detailed_scores\": [],  # Store detailed metric scores\n",
    "        }\n",
    "        \n",
    "        # Step counter for eval frequency\n",
    "        self.global_step = 0\n",
    "        self.eval_frequency = config.get(\"eval_frequency\", 5)\n",
    "        \n",
    "        print(f\"\u2705 GRPO Trainer initialized\")\n",
    "        print(f\"   Eval frequency: Every {self.eval_frequency} steps\")\n",
    "    \n",
    "    def generate_responses(self, prompts: List[str], num_generations: int) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Generate multiple responses for each prompt.\n",
    "        Returns: List of lists, where each inner list contains multiple generations for one prompt.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        all_generations = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in prompts:\n",
    "                # Format prompt for the model\n",
    "                formatted_prompt = f\"<|user|>\\\\n{prompt}\\\\n<|assistant|>\\\\n\"\n",
    "                \n",
    "                inputs = self.tokenizer(\n",
    "                    formatted_prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                ).to(self.model.device)\n",
    "                \n",
    "                # Generate multiple outputs\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.config[\"max_length\"],\n",
    "                    num_return_sequences=num_generations,\n",
    "                    temperature=self.config[\"temperature\"],\n",
    "                    top_p=self.config[\"top_p\"],\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "                \n",
    "                # Decode generations\n",
    "                generations = []\n",
    "                for output in outputs:\n",
    "                    decoded = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "                    # Extract only the assistant's response\n",
    "                    if \"<|assistant|>\" in decoded:\n",
    "                        decoded = decoded.split(\"<|assistant|>\")[-1].strip()\n",
    "                    generations.append(decoded)\n",
    "                \n",
    "                all_generations.append(generations)\n",
    "        \n",
    "        return all_generations\n",
    "    \n",
    "    def compute_advantages(self, rewards: List[float]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute group-relative advantages.\n",
    "        In GRPO, we normalize rewards within each group (multiple generations per prompt).\n",
    "        \"\"\"\n",
    "        rewards = np.array(rewards)\n",
    "        mean_reward = np.mean(rewards)\n",
    "        std_reward = np.std(rewards) + 1e-8\n",
    "        advantages = (rewards - mean_reward) / std_reward\n",
    "        return advantages.tolist()\n",
    "    \n",
    "    def compute_loss(self, prompts: List[str], generations: List[str], advantages: List[float]):\n",
    "        \"\"\"\n",
    "        Compute the GRPO loss.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for prompt, generation, advantage in zip(prompts, generations, advantages):\n",
    "            # Format input\n",
    "            formatted_text = f\"<|user|>\\\\n{prompt}\\\\n<|assistant|>\\\\n{generation}\"\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                formatted_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=1024\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            \n",
    "            # Weight loss by advantage\n",
    "            weighted_loss = outputs.loss * advantage\n",
    "            total_loss += weighted_loss\n",
    "        \n",
    "        return total_loss / len(prompts)\n",
    "    \n",
    "    def train_step(self, batch_prompts: List[str], batch_references: List[str], evaluate_this_step: bool = False):\n",
    "        \"\"\"\n",
    "        Single training step for GRPO.\n",
    "        \n",
    "        Args:\n",
    "            batch_prompts: List of input prompts\n",
    "            batch_references: List of reference outputs (ground truth)\n",
    "            evaluate_this_step: Whether to call evaluation API this step\n",
    "        \"\"\"\n",
    "        self.global_step += 1\n",
    "        \n",
    "        # Step 1: Generate multiple responses per prompt\n",
    "        all_generations = self.generate_responses(\n",
    "            batch_prompts,\n",
    "            self.config[\"num_generations_per_prompt\"]\n",
    "        )\n",
    "        \n",
    "        # Flatten generations\n",
    "        flat_prompts = []\n",
    "        flat_generations = []\n",
    "        flat_references = []\n",
    "        \n",
    "        for prompt, generations, reference in zip(batch_prompts, all_generations, batch_references):\n",
    "            for generation in generations:\n",
    "                flat_prompts.append(prompt)\n",
    "                flat_generations.append(generation)\n",
    "                flat_references.append(reference)\n",
    "        \n",
    "        # Step 2: Compute rewards \n",
    "        all_rewards = []\n",
    "        all_scores = None\n",
    "        \n",
    "        if evaluate_this_step:\n",
    "            # Call evaluation API (silently for progress bar)\n",
    "            start_time = time.time()\n",
    "            all_rewards, all_scores = self.reward_model.compute_batch_rewards(\n",
    "                flat_prompts, flat_generations, flat_references\n",
    "            )\n",
    "        else:\n",
    "            # Use simple heuristic rewards (no API call)\n",
    "            for gen, ref in zip(flat_generations, flat_references):\n",
    "                # Simple length-based reward as proxy\n",
    "                len_ratio = len(gen.split()) / max(len(ref.split()), 1)\n",
    "                reward = 1.0 - abs(1.0 - len_ratio)\n",
    "                reward = max(0, min(1, reward)) * 0.5  # Scale down heuristic rewards\n",
    "                all_rewards.append(reward)\n",
    "        \n",
    "        # Step 3: Compute group-relative advantages\n",
    "        advantages = self.compute_advantages(all_rewards)\n",
    "        \n",
    "        # Step 4: Update model using advantages\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.compute_loss(flat_prompts, flat_generations, advantages)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"mean_reward\": np.mean(all_rewards),\n",
    "            \"max_reward\": np.max(all_rewards),\n",
    "            \"min_reward\": np.min(all_rewards),\n",
    "            \"detailed_scores\": all_scores,\n",
    "            \"evaluated\": evaluate_this_step\n",
    "        }\n",
    "    \n",
    "    def train(self, dataset, num_epochs):\n",
    "        \"\"\"\n",
    "        Main training loop with optimized evaluation frequency and progress bars.\n",
    "        \"\"\"\n",
    "        print(f\"\\\\n{'='*80}\")\n",
    "        print(f\"\ud83d\ude80 Starting GRPO Training\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\ud83d\udcca Epochs: {num_epochs}\")\n",
    "        print(f\"\ud83d\udce6 Dataset size: {len(dataset)}\")\n",
    "        print(f\"\ud83d\udd22 Batch size: {self.config['batch_size']}\")\n",
    "        print(f\"\u23f1\ufe0f  Eval frequency: Every {self.eval_frequency} steps\")\n",
    "        print(f\"\ud83c\udfb2 Generations per prompt: {self.config['num_generations_per_prompt']}\")\n",
    "        print(f\"{'='*80}\\\\n\")\n",
    "        \n",
    "        # Create epoch progress bar\n",
    "        epoch_pbar = tqdm(range(num_epochs), desc=\"\ud83d\udcda Epochs\", position=0, leave=True)\n",
    "        \n",
    "        for epoch in epoch_pbar:\n",
    "            epoch_pbar.set_description(f\"\ud83d\udcda Epoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            epoch_metrics = {\n",
    "                \"loss\": [],\n",
    "                \"mean_reward\": [],\n",
    "                \"max_reward\": [],\n",
    "            }\n",
    "            \n",
    "            # Calculate number of batches\n",
    "            num_batches = (len(dataset) + self.config[\"batch_size\"] - 1) // self.config[\"batch_size\"]\n",
    "            \n",
    "            # Create step progress bar for this epoch\n",
    "            step_pbar = tqdm(\n",
    "                range(0, len(dataset), self.config[\"batch_size\"]),\n",
    "                desc=f\"  \ud83d\udd04 Steps\",\n",
    "                position=1,\n",
    "                leave=False,\n",
    "                total=num_batches\n",
    "            )\n",
    "            \n",
    "            for i in step_pbar:\n",
    "                batch = dataset[i:i + self.config[\"batch_size\"]]\n",
    "                batch_prompts = batch[\"prompt\"]\n",
    "                batch_references = batch[\"reference\"]\n",
    "                \n",
    "                # Determine if we should evaluate this step\n",
    "                evaluate_this_step = (self.global_step % self.eval_frequency == 0)\n",
    "                \n",
    "                # Training step (without verbose prints)\n",
    "                metrics = self.train_step(batch_prompts, batch_references, evaluate_this_step)\n",
    "                \n",
    "                # Track metrics\n",
    "                for key in epoch_metrics:\n",
    "                    if key in metrics:\n",
    "                        epoch_metrics[key].append(metrics[key])\n",
    "                \n",
    "                # Update progress bar with metrics\n",
    "                eval_marker = \"\ud83d\udcca EVAL\" if evaluate_this_step else \"\u26a1 FAST\"\n",
    "                step_pbar.set_postfix({\n",
    "                    'type': eval_marker,\n",
    "                    'loss': f\"{metrics['loss']:.4f}\",\n",
    "                    'reward': f\"{metrics['mean_reward']:.4f}\",\n",
    "                    'max': f\"{metrics['max_reward']:.4f}\",\n",
    "                    'global': self.global_step\n",
    "                })\n",
    "                \n",
    "                # Store detailed scores if evaluated\n",
    "                if metrics.get('detailed_scores'):\n",
    "                    self.metrics['detailed_scores'].append({\n",
    "                        'step': self.global_step,\n",
    "                        'scores': metrics['detailed_scores']\n",
    "                    })\n",
    "            \n",
    "            # Close step progress bar\n",
    "            step_pbar.close()\n",
    "            \n",
    "            # Epoch summary\n",
    "            epoch_loss = np.mean(epoch_metrics[\"loss\"])\n",
    "            epoch_mean_reward = np.mean(epoch_metrics[\"mean_reward\"])\n",
    "            epoch_max_reward = np.mean(epoch_metrics[\"max_reward\"])\n",
    "            \n",
    "            # Update epoch progress bar with summary\n",
    "            epoch_pbar.set_postfix({\n",
    "                'loss': f\"{epoch_loss:.4f}\",\n",
    "                'reward': f\"{epoch_mean_reward:.4f}\",\n",
    "                'max': f\"{epoch_max_reward:.4f}\"\n",
    "            })\n",
    "            \n",
    "            # Save metrics\n",
    "            self.metrics[\"epoch\"].append(epoch + 1)\n",
    "            self.metrics[\"loss\"].append(epoch_loss)\n",
    "            self.metrics[\"mean_reward\"].append(epoch_mean_reward)\n",
    "            self.metrics[\"max_reward\"].append(epoch_max_reward)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_dir = f\"{self.output_dir}/checkpoint-epoch-{epoch + 1}\"\n",
    "            self.save_checkpoint(checkpoint_dir)\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"\\\\n\ud83d\udcca Epoch {epoch + 1} Summary: Loss={epoch_loss:.4f}, Reward={epoch_mean_reward:.4f}, Max={epoch_max_reward:.4f}\")\n",
    "        \n",
    "        # Close epoch progress bar\n",
    "        epoch_pbar.close()\n",
    "        \n",
    "        print(f\"\\\\n{'='*80}\")\n",
    "        print(\"\u2705 Training Complete!\")\n",
    "        print(f\"{'='*80}\\\\n\")\n",
    "        \n",
    "        # Final stats\n",
    "        self.reward_model.print_stats()\n",
    "    \n",
    "    def save_checkpoint(self, checkpoint_dir):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(checkpoint_dir)\n",
    "        self.tokenizer.save_pretrained(checkpoint_dir)\n",
    "        \n",
    "        # Save metrics\n",
    "        with open(f\"{checkpoint_dir}/metrics.json\", \"w\") as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "\n",
    "print(\"\u2705 Optimized GRPO Trainer class defined with progress bars\")\n"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Optimized GRPO Trainer class defined with progress bars\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Cost Optimization Summary\n",
    "\n",
    "**With Evaluation Every N Steps:**\n",
    "- \u26a1 **Non-evaluation steps**: Use simple heuristic rewards (length-based)\n",
    "- \ud83d\udcca **Evaluation steps** (every 5th step): Call full API with priority metrics\n",
    "\n",
    "**Example Cost Savings:**\n",
    "```\n",
    "Original: Every step evaluation\n",
    "- 100 training steps \u00d7 16 generations \u00d7 2 metrics = 3,200 API calls\n",
    "\n",
    "Optimized: Every 5th step evaluation  \n",
    "- 20 evaluation steps \u00d7 16 generations \u00d7 2 metrics = 640 API calls\n",
    "- Savings: 80% reduction in API calls! \ud83c\udf89\n",
    "```\n",
    "\n",
    "**Caching Benefits:**\n",
    "- Repeated (prompt, output, reference) combinations are cached\n",
    "- No duplicate API calls for identical inputs\n",
    "- Automatic cache hit tracking\n",
    "\n",
    "**You can adjust:**\n",
    "- `EVAL_FREQUENCY` - Higher = fewer API calls, Lower = more frequent evaluation\n",
    "- `PRIORITY_METRICS` vs `ALL_METRICS` - Priority uses 2 metrics, All uses 6\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Note: The optimized GRPOTrainer is defined in Cell 14 above\n",
    "# This cell is intentionally empty - use the trainer from Cell 14\n",
    "\n",
    "print(\"\u2705 Using optimized GRPO Trainer from Cell 14\")\n"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Using optimized GRPO Trainer from Cell 14\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize and Run Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Initialize trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    reward_model=reward_model,\n",
    "    config=GRPO_CONFIG,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")\n"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 GRPO Trainer initialized\n",
      "   Eval frequency: Every 5 steps\n",
      "Trainer initialized\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Start training\n",
    "trainer.train(\n",
    "    dataset=train_dataset,\n",
    "    num_epochs=GRPO_CONFIG[\"num_train_epochs\"]\n",
    ")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\n================================================================================\n",
      "\ud83d\ude80 Starting GRPO Training\n",
      "================================================================================\n",
      "\ud83d\udcca Epochs: 3\n",
      "\ud83d\udce6 Dataset size: 9646\n",
      "\ud83d\udd22 Batch size: 4\n",
      "\u23f1\ufe0f  Eval frequency: Every 5 steps\n",
      "\ud83c\udfb2 Generations per prompt: 2\n",
      "================================================================================\\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96ab9ee87534c34b807f296e792fa13",
       "version_minor": 0.0,
       "version_major": 2.0
      },
      "text/plain": "\ud83d\udcda Epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33e4319bad347d68350b60eb982eba5",
       "version_minor": 0.0,
       "version_major": 2.0
      },
      "text/plain": "  \ud83d\udd04 Steps:   0%|          | 0/2412 [00:00<?, ?it/s]"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# IMPORTANT: Set your Hugging Face token as an environment variable\n",
    "# Never commit tokens to code!\n",
    "# export HF_TOKEN=your_hugging_face_token_here\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    print(\"\u274c HF_TOKEN environment variable not set!\")\n",
    "    print(\"Please set it with: export HF_TOKEN=your_token_here\")\n",
    "    print(\"Get your token from: https://huggingface.co/settings/tokens\")\n",
    "else:\n",
    "    login(hf_token)\n",
    "    print(\"\u2705 Successfully logged in to Hugging Face\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(trainer.metrics[\"epoch\"], trainer.metrics[\"loss\"], marker='o')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Reward plot\n",
    "axes[1].plot(trainer.metrics[\"epoch\"], trainer.metrics[\"mean_reward\"], marker='o', label='Mean Reward')\n",
    "axes[1].plot(trainer.metrics[\"epoch\"], trainer.metrics[\"max_reward\"], marker='s', label='Max Reward')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Reward')\n",
    "axes[1].set_title('Training Rewards')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/training_metrics.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Metrics plot saved to {OUTPUT_DIR}/training_metrics.png\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "def test_model(model, tokenizer, prompt: str, max_length: int = 512):\n",
    "    \"\"\"\n",
    "    Test the trained model on a single prompt.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Format prompt\n",
    "    formatted_prompt = f\"<|user|>\\\\n{prompt}\\\\n<|assistant|>\\\\n\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract assistant's response\n",
    "    if \"<|assistant|>\" in generated_text:\n",
    "        generated_text = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "    return generated_text\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Test on a few examples\n",
    "test_prompts = [\n",
    "    \"Please rewrite the following text in my personal style: The sun was setting over the horizon.\",\n",
    "    \"Transform this sentence to match my writing style: Technology has changed our lives.\",\n",
    "    \"Rewrite in my style: The meeting was very productive and we made good progress.\"\n",
    "]\n",
    "\n",
    "print(\"Testing the trained model:\\\\n\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Test {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"\\\\nGenerated Output:\")\n",
    "    output = test_model(model, tokenizer, prompt)\n",
    "    print(output)\n",
    "    print()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Final Model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Save final model\n",
    "final_model_dir = f\"{OUTPUT_DIR}/final_model\"\n",
    "os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "print(f\"Final model saved to {final_model_dir}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comprehensive Final Evaluation (All Metrics)\n",
    "\n",
    "After training with priority metrics, evaluate the model using **all 6 metrics** for a complete assessment:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Create a comprehensive evaluation reward model with all metrics\n",
    "comprehensive_reward_model = APIIntegratedRewardModel(\n",
    "    api_base_url=API_BASE_URL,\n",
    "    metrics=ALL_METRICS,  # Use all 6 metrics\n",
    "    weights=REWARD_WEIGHTS,\n",
    "    api_config=API_CONFIG,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "# Test on a few examples with comprehensive evaluation\n",
    "test_examples = [\n",
    "    {\n",
    "        \"prompt\": \"Please rewrite the following text in my personal style: The sun was setting over the horizon.\",\n",
    "        \"reference\": \"Your expected reference output here\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Transform this sentence to match my writing style: Technology has changed our lives.\",\n",
    "        \"reference\": \"Your expected reference output here\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(\"Comprehensive Final Evaluation (All 6 Metrics)\")\n",
    "print(f\"{'='*80}\\\\n\")\n",
    "\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    print(f\"\\\\n{'\u2500'*80}\")\n",
    "    print(f\"Example {i}\")\n",
    "    print(f\"{'\u2500'*80}\")\n",
    "    print(f\"Prompt: {example['prompt']}\")\n",
    "\n",
    "    # Generate output\n",
    "    output = test_model(model, tokenizer, example['prompt'])\n",
    "    print(f\"\\\\nGenerated Output:\\\\n{output}\")\n",
    "\n",
    "    # Evaluate with all metrics\n",
    "    print(f\"\\\\n{'\u2500'*40}\")\n",
    "    print(\"Evaluation Scores:\")\n",
    "    print(f\"{'\u2500'*40}\")\n",
    "\n",
    "    rewards, scores = comprehensive_reward_model.compute_batch_rewards(\n",
    "        [example['prompt']],\n",
    "        [output],\n",
    "        [example['reference']]\n",
    "    )\n",
    "\n",
    "    # Display individual metric scores\n",
    "    for metric in ALL_METRICS:\n",
    "        if metric in scores[0]:\n",
    "            score = scores[0][metric]\n",
    "            print(f\"  {metric:.<30} {score}/5\")\n",
    "\n",
    "    print(f\"\\\\n  {'Overall Weighted Reward':.<30} {rewards[0]:.4f}/1.0\")\n",
    "    print(f\"{'\u2500'*80}\\\\n\")\n",
    "\n",
    "# Print final API statistics\n",
    "comprehensive_reward_model.print_stats()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load and Use Trained Model (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Load the trained model later with Unsloth\n",
    "def load_trained_model(adapter_path: str, max_seq_length: int = 2048):\n",
    "    \"\"\"\n",
    "    Load the trained model with LoRA adapters using Unsloth.\n",
    "\n",
    "    Args:\n",
    "        adapter_path: Path to the saved checkpoint/adapter\n",
    "        max_seq_length: Maximum sequence length\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (model, tokenizer)\n",
    "    \"\"\"\n",
    "    print(f\"Loading trained model from: {adapter_path}\")\n",
    "\n",
    "    # Unsloth can load the model with adapters directly\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=adapter_path,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    # Enable inference mode for faster generation\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    print(\"\u2705 Trained model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Example usage:\n",
    "# trained_model, trained_tokenizer = load_trained_model(final_model_dir)\n",
    "# or load from checkpoint:\n",
    "# trained_model, trained_tokenizer = load_trained_model(\"./grpo_style_transfer_model/checkpoint-epoch-3\")\n",
    "\n",
    "print(\"\u2705 Model loading function defined (Unsloth optimized)\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}